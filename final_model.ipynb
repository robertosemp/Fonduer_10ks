{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonduer applied to financial statement processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data download\n",
    "\n",
    "We have downloaded about 2000 html files into a directory. The download was carried out through Ubuntu's command prompt with 'sudo wget <link>'\n",
    "\n",
    "#### Creating postgresql\n",
    "\n",
    "Also created a postgresql database called which doesnt have any files in it\n",
    "\n",
    "#### Populating database (KBC initialization)\n",
    "we need to create a schema within the postgresql database to store our files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import psutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import math\n",
    "#from personal_utils import gen_custom_marginals\n",
    "\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "\n",
    "#ATTRIBUTE is the name of the database\n",
    "PARALLEL = 32\n",
    "ATTRIBUTE = \"test_may_29\" # 2,007 docs database\n",
    "#ATTRIBUTE = \"finance_may_small\" # 50 docs database\n",
    "conn_string = 'postgresql://localhost:5432/' + ATTRIBUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking pickles\n",
    "try:\n",
    "    pickle_in = open(\"pickle.L_Train_full\", \"rb\")\n",
    "    pickled = True\n",
    "except:\n",
    "    pickled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiliazing fonduer KB\n",
    "\n",
    "We first initialize a Meta object, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-09 18:19:19,164][INFO] fonduer.meta:50 - Setting logging directory to: logs/2019-06-09_18-19-19\n",
      "[2019-06-09 18:19:19,200][INFO] fonduer.meta:134 - Connecting user:None to localhost:5432/test_may_29\n",
      "[2019-06-09 18:19:20,079][INFO] fonduer.meta:161 - Initializing the storage schema\n"
     ]
    }
   ],
   "source": [
    "from fonduer import Meta, init_logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "init_logging(log_dir=\"logs\")\n",
    "\n",
    "session = Meta.init(conn_string).Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing the docs\n",
    "\n",
    "Next, we load the corpus of 10-Q's and transform them into the unified data model. Each doc is represented by an HTM file. The HTM file is parsed to create a robust unified data model with textual, structural, and tabular modality information. Note that since each document is independent of each other, we can parse the documents in parallel. We depend on PostgreSQL for this functionality.\n",
    "\n",
    "HTMLDocPreprocessor(DocPreprocessor):A generator which processes an HTML file or directory of HTML files into a set of Document objects.\n",
    "    \n",
    "Next, we configure a Parser, which serves as our CorpusParser for PDF documents. We use spaCy as a preprocessing tool to split our documents into sentences and tokens.In addition, we can specify which modality information to include in the unified data model for each document. Below, we enable structural information, as well as lingual information, which uses [spaCy] to provide annotations such as part-of-speech tags and dependency parse structures for these sentences. Note that after the progress bar indicates the completion of the parsing process, some more time will pass until all objects have been inserted into the database.\n",
    "\n",
    "*If you get an error that the path is wrong it may be that the files within the path are not html and so the command doesnt work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser import Parser\n",
    "\n",
    "docs_path = \"/home/ubuntu/anaconda3/finance_test/finance_may_24\"\n",
    "doc_preprocessor = HTMLDocPreprocessor(docs_path)\n",
    "\n",
    "corpus_parser = Parser(session, structural=True, lingual=True, parallelism=PARALLEL)\n",
    "%time corpus_parser.apply(doc_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 2007\n",
      "Sentences: 2956776\n",
      "Paragraphs: 4391717\n",
      "Figures: 2726\n",
      "Tables: 291815\n",
      "Context: 291815\n"
     ]
    }
   ],
   "source": [
    "from fonduer.parser.models import Document, Sentence, Paragraph, Figure, Table, Context\n",
    "\n",
    "print(f\"Documents: {session.query(Document).count()}\")\n",
    "print(f\"Sentences: {session.query(Sentence).count()}\")\n",
    "print(f\"Paragraphs: {session.query(Paragraph).count()}\")\n",
    "print(f\"Figures: {session.query(Figure).count()}\")\n",
    "print(f\"Tables: {session.query(Table).count()}\")\n",
    "print(f\"Context: {session.query(Table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dividing the Corpus into Test and Train¶\n",
    "\n",
    "We'll split the documents 80/10/10 into train/dev/test splits. Note that here we do this in a non-random order to preverse the consistency in the tutorial, and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sqlalchemy.sql.expression import func, select\n",
    "\n",
    "docs = session.query(Document).all()\n",
    "ld = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs = set()\n",
    "test_docs = set()\n",
    "splits = (0.7, 0.85)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "\n",
    "#pprint([x.name for x in train_docs])\n",
    "#pprint([x.name for x in dev_docs])\n",
    "#pprint([x.name for x in test_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mention extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import mention_subclass\n",
    "import re\n",
    "from fonduer.utils.data_model_utils import *\n",
    "\n",
    "period = mention_subclass(\"period\")\n",
    "revenue = mention_subclass(\"revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.matchers import DictionaryMatch, RegexMatchSpan\n",
    "\n",
    "date_matcher = RegexMatchSpan(rgx=r'(January|February|March|April|May|June|July|August|September|October|November|December)(\\s)(28|29|30|31)(,)', longest_match_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mention_span_matches_integer(mention):\n",
    "    potential_integer_string = mention.get_span()\n",
    "    try:\n",
    "        type(int(potential_integer_string[0])) == int\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_table(mention):\n",
    "    if (mention.sentence.table != None): return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_whitespace(mention):\n",
    "    tracker = True\n",
    "    for letter in mention.get_span()[:]:\n",
    "        if letter == ' ':\n",
    "            tracker = False\n",
    "            break\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_exclude(mention):\n",
    "    exclude_words = ['cogs', 'COGS', 'cost', 'costs', 'Cost', 'Costs','net income', \n",
    "                     'other', 'Other','deferred', 'Deferred', 'unearned','Unearned']\n",
    "    words_in_row = get_row_ngrams(mention)\n",
    "    for item in words_in_row:\n",
    "        if item in exclude_words:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_has_revenue(mention):\n",
    "    revenue_words = ['revenue','revenues', 'Revenue', 'Revenues', 'REVENUE', 'REVENUES',\n",
    "                    'sales', 'Sales', 'SALES']\n",
    "    words_in_row = get_row_ngrams(mention)\n",
    "    for item in words_in_row:\n",
    "        if item in revenue_words:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_length(mention):\n",
    "    if len(mention.get_span()[:]) <= 3:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to check recall. Some real candidates are not being included. Maybe because of 'EndedMarch, 30' \n",
    "#in some case without recognizing March\n",
    "\n",
    "from fonduer.candidates.matchers import LambdaFunctionMatcher, Intersect, Union\n",
    "\n",
    "#Creating Common matchers\n",
    "in_table_matcher = LambdaFunctionMatcher(func = is_in_table)\n",
    "\n",
    "#Creating all revenue matchers\n",
    "integer_matcher = LambdaFunctionMatcher(func = mention_span_matches_integer)\n",
    "no_whitespace_matcher = LambdaFunctionMatcher(func = no_whitespace)\n",
    "row_has_revenue_matcher = LambdaFunctionMatcher(func = row_has_revenue)\n",
    "words_exclude_matcher = LambdaFunctionMatcher(func = words_exclude)\n",
    "min_length_matcher = LambdaFunctionMatcher(func = min_length)\n",
    "revenue_matcher = Intersect(integer_matcher, in_table_matcher, no_whitespace_matcher, \n",
    "                            row_has_revenue_matcher, words_exclude_matcher, min_length_matcher)\n",
    "\n",
    "\n",
    "# Creating all period matchers\n",
    "period_matcher = Intersect(date_matcher, in_table_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Mention's MentionSpace¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionNgrams\n",
    "\n",
    "period_ngrams = MentionNgrams(n_max=6)\n",
    "revenue_ngrams = MentionNgrams(n_max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Mention Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionExtractor\n",
    "\n",
    "\n",
    "mention_extractor = MentionExtractor(\n",
    "    session,\n",
    "    [period, revenue],\n",
    "    [period_ngrams, revenue_ngrams],\n",
    "    [period_matcher, revenue_matcher],\n",
    "    parallelism = PARALLEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import Mention\n",
    "\n",
    "mention_extractor.apply(docs, parallelism = PARALLEL)\n",
    "num_periods = session.query(period).count()\n",
    "num_revenues = session.query(revenue).count()\n",
    "print(\n",
    "    f\"Total Mentions: {session.query(Mention).count()} ({num_periods} period, {num_revenues} revenue)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throttlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_throttler(c):\n",
    "    return same_table(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import candidate_subclass\n",
    "\n",
    "period_revenue = candidate_subclass(\"period_revenue\", [period, revenue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-09 18:19:55,974][INFO] fonduer.candidates.candidates:125 - Clearing table period_revenue (split 0)\n",
      "[2019-06-09 18:19:56,548][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a02049d26c453ebb62e03a8986d7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1405), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-09 18:20:20,200][INFO] fonduer.candidates.candidates:125 - Clearing table period_revenue (split 1)\n",
      "[2019-06-09 18:20:20,311][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Candidates in split=0: 12948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1854cdad926545a2b69a494ecc569686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=301), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-09 18:20:33,058][INFO] fonduer.candidates.candidates:125 - Clearing table period_revenue (split 2)\n",
      "[2019-06-09 18:20:33,135][INFO] fonduer.utils.udf:54 - Running UDF...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Candidates in split=1: 2529\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087c78a0fec14541af626c0ff35b988c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=301), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Candidates in split=2: 1740\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates.models import candidate_subclass\n",
    "from fonduer.candidates import CandidateExtractor\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "#set_trace()\n",
    "candidate_extractor = CandidateExtractor(session, \n",
    "                                         [period_revenue],\n",
    "                                         throttlers = [final_throttler],\n",
    "                                         self_relations = False,\n",
    "                                         nested_relations = False,\n",
    "                                         symmetric_relations = True)\n",
    "\n",
    "\n",
    "for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i, parallelism=PARALLEL)\n",
    "    print(f\"Number of Candidates in split={i}: {session.query(period_revenue).filter(period_revenue.split == i).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = candidate_extractor.get_candidates(split = 0)\n",
    "dev_cands = candidate_extractor.get_candidates(split = 1)\n",
    "test_cands = candidate_extractor.get_candidates(split = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feauturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12948, 110096)\n"
     ]
    }
   ],
   "source": [
    "#if train_cands generates error such as 'period' doesn't have attribute ..., try [train_cands]\n",
    "from fonduer.features import Featurizer\n",
    "try:\n",
    "    pickle_in = open(\"pickle.F_Train_full\", \"rb\")\n",
    "    #pickle_in = open(\"pickle.F_Train\", \"rb\")\n",
    "    F_train_dense = pickle.load(pickle_in)\n",
    "    F_train = [sparse.csr_matrix(F_train_dense)]\n",
    "except:\n",
    "    featurizer = Featurizer(session, [period_revenue])\n",
    "    %time featurizer.apply(split=0, train=True, parallelism=PARALLEL)\n",
    "    %time F_train = featurizer.get_feature_matrices(train_cands)\n",
    "print(F_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2529, 110096)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pickle_in = open(\"pickle.F_dev_full\", \"rb\")\n",
    "    #pickle_in = open(\"pickle.F_dev\", \"rb\")\n",
    "    F_dev_dense = pickle.load(pickle_in)\n",
    "    F_dev = [sparse.csr_matrix(F_dev_dense)]\n",
    "except:\n",
    "    %time featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "    %time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "print(F_dev[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1740, 110096)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pickle_in = open(\"pickle.F_test_full\", \"rb\")\n",
    "    #pickle_in = open(\"pickle.F_test\", \"rb\")\n",
    "    F_test_dense = pickle.load(pickle_in)\n",
    "    F_test = [sparse.csr_matrix(F_test_dense)]\n",
    "except:\n",
    "    %time featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "    %time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "print(F_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labeling functions\n",
    "\n",
    "LF are not immediate eliminators of candidates (as are throttlers). For labeling functions, there is a probabilistic analysis that determines which candidates are the most likely to be correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN = 0\n",
    "FALSE = 1\n",
    "TRUE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_same_page(c):\n",
    "    (period, revenue) = c\n",
    "    if same_page(c): return TRUE\n",
    "    else: ABSTAIN\n",
    "\n",
    "def LF_vertical_align(c):\n",
    "    (period, revenue) = c\n",
    "    per = period.context.sentence\n",
    "    rev = revenue.context.sentence\n",
    "    if per.col_start <= rev.col_start and per.col_end >= rev.col_end: return TRUE\n",
    "    else: return ABSTAIN\n",
    "\n",
    "def LF_early_in_table(c):\n",
    "    (period, revenue) = c\n",
    "    per = period.context.sentence\n",
    "    rev = revenue.context.sentence\n",
    "    if per.row_start <= 3 and rev.row_start <= 12: return TRUE\n",
    "    else: return ABSTAIN\n",
    "    \n",
    "def LF_period_bold(c):\n",
    "    (period, revenue) = c\n",
    "    per = period.context.sentence\n",
    "    if \"font-weight:bold\" in per.html_attrs[0]: return TRUE\n",
    "    else: return ABSTAIN \n",
    "        \n",
    "def LF_common_ancestor(c):\n",
    "    if common_ancestor(c) != None: return FALSE\n",
    "    else: return ABSTAIN\n",
    "    \n",
    "def LF_lowest_common_ancestor_depth(c):\n",
    "    if lowest_common_ancestor_depth(c) <= 3: return TRUE\n",
    "    else: return ABSTAIN\n",
    "    \n",
    "def LF_near_row(c):\n",
    "    (period, revenue) = c\n",
    "    if revenue.context.sentence.row_start - period.context.sentence.row_start <= 10: return TRUE\n",
    "    else: return ABSTAIN\n",
    "    \n",
    "\n",
    "def LF_header_not_date(c):\n",
    "    (period, revenue) = c\n",
    "    if not overlap(list(get_head_ngrams(mention = revenue, axis = 'col')), \n",
    "                   period.context.sentence.words): \n",
    "        return FALSE\n",
    "    else: \n",
    "        return ABSTAIN\n",
    "\n",
    "def LF_early_table(c):\n",
    "    (period, revenue) = c\n",
    "    per = period.context.sentence\n",
    "    table_pos = per.table.position\n",
    "    sentence_pos = per.position\n",
    "    if table_pos > 0:\n",
    "        ratio = sentence_pos / table_pos\n",
    "        if table_pos <= 10 and ratio > 10:\n",
    "            return TRUE\n",
    "        else:\n",
    "            return ABSTAIN\n",
    "\n",
    "\n",
    "def LF_no_point(c):\n",
    "    (period, revenue) = c\n",
    "    if len(revenue.context.sentence.text[:]) < 5:\n",
    "        for letter in revenue.context.sentence.text[:]:\n",
    "            if letter == '.': \n",
    "                return FALSE\n",
    "        return ABSTAIN\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "def LF_no_perc(c):\n",
    "    (period, revenue) = c\n",
    "    for letter in revenue.context.sentence.text[:]:\n",
    "        if letter == '%': return FALSE\n",
    "    return ABSTAIN\n",
    "\n",
    "def LF_revenue_length(c):\n",
    "    (period, revenue) = c\n",
    "    has_dot = False\n",
    "    for letter in revenue.context.sentence.text[:]:\n",
    "        if letter == '.':\n",
    "            has_dot = True\n",
    "            \n",
    "    if has_dot:\n",
    "        if len(revenue.context.sentence.text[:]) <= 4:\n",
    "            return FALSE\n",
    "    else:\n",
    "        if len(revenue.context.sentence.text[:]) <= 3:\n",
    "            return FALSE\n",
    "        elif type(revenue.context.sentence.text) == int:\n",
    "            return FALSE\n",
    "        \n",
    "    return ABSTAIN\n",
    "            \n",
    "    \n",
    "def LF_rev_not_like_year(c):\n",
    "    (period, revenue) = c\n",
    "    rev = revenue.context.sentence\n",
    "    \n",
    "    has_dot = False\n",
    "    for letter in rev.text[:]:\n",
    "        if letter == '.':\n",
    "            has_dot = True\n",
    "            break\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        fig = int(rev.text)\n",
    "        if (fig == 2018 | fig == 2019 | fig == 2017) == True & has_dot == False:\n",
    "            return FALSE\n",
    "    except:\n",
    "        return ABSTAIN\n",
    "    return ABSTAIN\n",
    "\n",
    "def LF_tot_revenues(c):\n",
    "    \n",
    "    (period, revenue) = c\n",
    "    rev = revenue.context.sentence\n",
    "    words_row = get_row_ngrams(revenue)\n",
    "    \n",
    "    total_words = ['total', 'Total', 'TOTAL']\n",
    "    revenue_words = ['revenue','revenues', 'Revenue', 'Revenues', 'REVENUE', 'REVENUES',\n",
    "                     'sales', 'Sales', 'SALES']\n",
    "    \n",
    "    if overlap(words_row, total_words) & overlap(words_row, revenue_words):\n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_PL_words(c):\n",
    "    (period, revenue) = c\n",
    "    \n",
    "    cost_words = ['cost', 'costs']\n",
    "    income_words = ['net', 'operating']\n",
    "    share_words = ['share']\n",
    "    expense_words = ['selling', 'administrative','general']\n",
    "    tax_words = ['tax', 'taxes']\n",
    "    \n",
    "        \n",
    "    #getting mention document and table data\n",
    "    doc_id = revenue.context.sentence.document_id\n",
    "    table_num = revenue.context.sentence.table_id\n",
    "    \n",
    "    #extracting all columns in the first sentence of that table\n",
    "    sentences = session.query(Sentence).filter(Sentence.document_id == doc_id, \n",
    "                                               Sentence.table_id == table_num, \n",
    "                                               Sentence.col_start == 0).order_by(Sentence.position)\n",
    "\n",
    "    #looking for specific words \n",
    "    c = 0\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.lower() in cost_words:\n",
    "                c += 1\n",
    "            if word.lower() in income_words:\n",
    "                c += 0.5\n",
    "            if word.lower() in share_words:\n",
    "                c += 1\n",
    "            if word.lower() in expense_words:\n",
    "                c += 0.5\n",
    "            if word.lower() in tax_words:\n",
    "                c += 2\n",
    "\n",
    "    if c >=4: return TRUE\n",
    "    else: return ABSTAIN\n",
    "\n",
    "def LF_big_table(c):\n",
    "    (period, revenue) = c\n",
    "    rev = revenue.context.sentence\n",
    "    doc_id = rev.document_id\n",
    "    table_num = rev.table_id\n",
    "    row_num = rev.row_start\n",
    "    \n",
    "    sentences = session.query(Sentence).filter(Sentence.document_id == doc_id,\n",
    "                                               Sentence.table_id == table_num,\n",
    "                                               Sentence.col_start >= 0).order_by(Sentence.position)\n",
    "    row = 0\n",
    "    for sentence in sentences:\n",
    "        row = sentence.row_start\n",
    "        \n",
    "    if row >= 20: return TRUE\n",
    "    else: return ABSTAIN\n",
    "    \n",
    "    \n",
    "def LF_revenue_max(c):\n",
    "    (period, revenue) = c\n",
    "    rev = revenue.context.sentence\n",
    "    doc_id = rev.document_id\n",
    "    table_num = rev.table_id\n",
    "    row_num = rev.row_start\n",
    "    col_beg = rev.col_start\n",
    "    col_end = rev.col_end\n",
    "    \n",
    "    sentences = session.query(Sentence).filter(Sentence.document_id == doc_id,\n",
    "                                               Sentence.table_id == table_num,\n",
    "                                               Sentence.col_start >= 0,\n",
    "                                               Sentence.col_start <= 1,\n",
    "                                               Sentence.row_start >= row_num + 1,\n",
    "                                               Sentence.row_start <= row_num + 5,\n",
    "                                               Sentence.row_start != row_num).order_by(Sentence.position)\n",
    "    \n",
    "    revenue_words = ['revenue','revenues', 'Revenue', 'Revenues', 'REVENUE', 'REVENUES',\n",
    "                    'sales', 'Sales', 'SALES']\n",
    "    \n",
    "    cost_words = ['cost', 'Cost', 'COST', 'costs', 'Costs', 'COSTS', \n",
    "                  'goods', 'Goods', 'GOODS' 'sold', 'Sold', 'SOLD']\n",
    "    \n",
    "    cost_count = 0\n",
    "    revenue_count = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence.words:\n",
    "            #print(word)\n",
    "            if word in revenue_words:\n",
    "                revenue_count += 1\n",
    "                #print('this is a revenue')\n",
    "            elif word in cost_words:\n",
    "                cost_count +=1\n",
    "                #print('this is a cost')\n",
    "     \n",
    "    if cost_count >0 and revenue_count <= 1:\n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    " \n",
    "\n",
    "        \n",
    "def LF_note_word(c):\n",
    "    (period, revenue) = c\n",
    "    per = period.context.sentence\n",
    "    doc_id = per.document_id\n",
    "    table_num = per.table_id\n",
    "    sentences = session.query(Sentence).filter(Sentence.document_id == doc_id, \n",
    "                                               Sentence.position <= per.position,\n",
    "                                               Sentence.position >= per.position - 10).order_by(Sentence.position)\n",
    "    \n",
    "    note_words = ['note', 'Note', 'NOTE', 'notes', 'Notes', 'NOTES'] \n",
    "\n",
    "    for sentence in sentences:\n",
    "        c = 0\n",
    "        for word in sentence.words:\n",
    "            if overlap(note_words, sentence.words):\n",
    "                return FALSE\n",
    "    return ABSTAIN\n",
    "\n",
    "    \n",
    "def LF_close_IS(c):\n",
    "    (period, revenue) = c\n",
    "    per = period.context.sentence\n",
    "    doc_id = per.document_id\n",
    "    table_num = per.table_id\n",
    "    sentences = session.query(Sentence).filter(Sentence.document_id == doc_id, \n",
    "                                               Sentence.position < per.position,\n",
    "                                               Sentence.position >= (per.position - 5)).order_by(Sentence.position)\n",
    "    \n",
    "    income_words = ['income', 'Income', 'INCOME', \n",
    "                    'operation', 'Operation', 'OPERATION', 'operations','Operations', 'OPERATIONS']\n",
    "    statement_words = ['statement', 'Statement', 'STATEMENT', 'statements', 'Statements', 'STATEMENTS']\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence.words:\n",
    "            if overlap(income_words, sentence.words) & overlap(statement_words, sentence.words):\n",
    "                return TRUE\n",
    "            else:\n",
    "                return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_parallel = [LF_vertical_align,\n",
    "               LF_early_in_table,\n",
    "               LF_near_row,\n",
    "               LF_rev_not_like_year,\n",
    "               LF_early_table,\n",
    "               LF_no_point,\n",
    "               LF_no_perc,\n",
    "               LF_tot_revenues,\n",
    "               LF_revenue_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_linear = [LF_PL_words,\n",
    "             LF_big_table,\n",
    "             LF_revenue_max,\n",
    "             LF_note_word,\n",
    "             LF_close_IS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFs = [LF_vertical_align,\n",
    "       LF_early_in_table,\n",
    "       LF_near_row,\n",
    "       LF_rev_not_like_year,\n",
    "       LF_early_table,\n",
    "       LF_no_point,\n",
    "       LF_no_perc,\n",
    "       LF_tot_revenues,\n",
    "       LF_revenue_length,\n",
    "       LF_PL_words,\n",
    "       LF_big_table,\n",
    "       LF_revenue_max,\n",
    "       LF_note_word,\n",
    "       LF_close_IS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFCs = [LF_same_page, \n",
    "       LF_vertical_align,\n",
    "       LF_early_in_table,\n",
    "       LF_period_bold,\n",
    "       LF_near_row,\n",
    "       LF_PL_words,\n",
    "       LF_rev_not_like_year,\n",
    "       LF_big_table,\n",
    "       LF_header_not_date,\n",
    "       LF_early_table,\n",
    "       LF_revenue_max,\n",
    "       LF_note_word,\n",
    "       LF_close_IS,\n",
    "       LF_no_point,\n",
    "       LF_revenue_length,\n",
    "       LF_no_perc,\n",
    "       LF_tot_revenues]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing or creating labels for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.supervision import Labeler\n",
    "try:\n",
    "    pickle_in = open(\"pickle.L_Train_full\", \"rb\")\n",
    "    #pickle_in = open(\"pickle.L_Train\", \"rb\")\n",
    "    L_train_dense = pickle.load(pickle_in)\n",
    "    L_train = [sparse.csr_matrix(L_train_dense)]\n",
    "    \n",
    "except:\n",
    "    try:\n",
    "        labeler.clear()\n",
    "        labeler = Labeler(session = session, candidate_classes = [period_revenue])\n",
    "    except:\n",
    "        labeler = Labeler(session = session, candidate_classes = [period_revenue])\n",
    "\n",
    "    %time labeler.apply(split=0, lfs=[LF_parallel], train=True, clear=True, parallelism = PARALLEL)\n",
    "    %time labeler.update(split=0, lfs=[LF_linear])\n",
    "    %time L_train = labeler.get_label_matrices(train_cands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fonduer.supervision import Labeler\n",
    "\n",
    "try:\n",
    "    labeler.clear()\n",
    "    labeler = Labeler(session = session, candidate_classes = [period_revenue])\n",
    "except:\n",
    "    labeler = Labeler(session = session, candidate_classes = [period_revenue])\n",
    "\n",
    "%time labeler.apply(split=0, lfs=[LF_parallel], train=True, clear=True, parallelism = PARALLEL)\n",
    "%time labeler.update(split=0, lfs=[LF_linear])\n",
    "%time L_train = labeler.get_label_matrices(train_cands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal import analysis\n",
    "\n",
    "if pickled: analysis.lf_summary(L_train[0])\n",
    "else: analysis.lf_summary(L_train[0], lf_names = labeler.get_keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Fonduer marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing O...\n",
      "Estimating \\mu...\n",
      "[1 epo]: TRAIN:[loss=1.969]\n",
      "[2 epo]: TRAIN:[loss=1.915]\n",
      "[3 epo]: TRAIN:[loss=1.811]\n",
      "[4 epo]: TRAIN:[loss=1.664]\n",
      "[5 epo]: TRAIN:[loss=1.480]\n",
      "[6 epo]: TRAIN:[loss=1.268]\n",
      "[7 epo]: TRAIN:[loss=1.038]\n",
      "[8 epo]: TRAIN:[loss=0.803]\n",
      "[9 epo]: TRAIN:[loss=0.583]\n",
      "[10 epo]: TRAIN:[loss=0.397]\n",
      "[11 epo]: TRAIN:[loss=0.265]\n",
      "[12 epo]: TRAIN:[loss=0.200]\n",
      "[13 epo]: TRAIN:[loss=0.203]\n",
      "[14 epo]: TRAIN:[loss=0.258]\n",
      "[15 epo]: TRAIN:[loss=0.337]\n",
      "[16 epo]: TRAIN:[loss=0.405]\n",
      "[17 epo]: TRAIN:[loss=0.434]\n",
      "[18 epo]: TRAIN:[loss=0.416]\n",
      "[19 epo]: TRAIN:[loss=0.361]\n",
      "[20 epo]: TRAIN:[loss=0.290]\n",
      "[21 epo]: TRAIN:[loss=0.223]\n",
      "[22 epo]: TRAIN:[loss=0.176]\n",
      "[23 epo]: TRAIN:[loss=0.153]\n",
      "[24 epo]: TRAIN:[loss=0.151]\n",
      "[25 epo]: TRAIN:[loss=0.162]\n",
      "[26 epo]: TRAIN:[loss=0.180]\n",
      "[27 epo]: TRAIN:[loss=0.196]\n",
      "[28 epo]: TRAIN:[loss=0.207]\n",
      "[29 epo]: TRAIN:[loss=0.210]\n",
      "[30 epo]: TRAIN:[loss=0.203]\n",
      "[31 epo]: TRAIN:[loss=0.190]\n",
      "[32 epo]: TRAIN:[loss=0.172]\n",
      "[33 epo]: TRAIN:[loss=0.151]\n",
      "[34 epo]: TRAIN:[loss=0.132]\n",
      "[35 epo]: TRAIN:[loss=0.117]\n",
      "[36 epo]: TRAIN:[loss=0.107]\n",
      "[37 epo]: TRAIN:[loss=0.103]\n",
      "[38 epo]: TRAIN:[loss=0.104]\n",
      "[39 epo]: TRAIN:[loss=0.109]\n",
      "[40 epo]: TRAIN:[loss=0.114]\n",
      "[41 epo]: TRAIN:[loss=0.119]\n",
      "[42 epo]: TRAIN:[loss=0.121]\n",
      "[43 epo]: TRAIN:[loss=0.119]\n",
      "[44 epo]: TRAIN:[loss=0.115]\n",
      "[45 epo]: TRAIN:[loss=0.108]\n",
      "[46 epo]: TRAIN:[loss=0.102]\n",
      "[47 epo]: TRAIN:[loss=0.096]\n",
      "[48 epo]: TRAIN:[loss=0.091]\n",
      "[49 epo]: TRAIN:[loss=0.088]\n",
      "[50 epo]: TRAIN:[loss=0.087]\n",
      "[51 epo]: TRAIN:[loss=0.087]\n",
      "[52 epo]: TRAIN:[loss=0.088]\n",
      "[53 epo]: TRAIN:[loss=0.088]\n",
      "[54 epo]: TRAIN:[loss=0.088]\n",
      "[55 epo]: TRAIN:[loss=0.088]\n",
      "[56 epo]: TRAIN:[loss=0.086]\n",
      "[57 epo]: TRAIN:[loss=0.084]\n",
      "[58 epo]: TRAIN:[loss=0.082]\n",
      "[59 epo]: TRAIN:[loss=0.080]\n",
      "[60 epo]: TRAIN:[loss=0.078]\n",
      "[61 epo]: TRAIN:[loss=0.076]\n",
      "[62 epo]: TRAIN:[loss=0.075]\n",
      "[63 epo]: TRAIN:[loss=0.075]\n",
      "[64 epo]: TRAIN:[loss=0.074]\n",
      "[65 epo]: TRAIN:[loss=0.074]\n",
      "[66 epo]: TRAIN:[loss=0.073]\n",
      "[67 epo]: TRAIN:[loss=0.072]\n",
      "[68 epo]: TRAIN:[loss=0.072]\n",
      "[69 epo]: TRAIN:[loss=0.070]\n",
      "[70 epo]: TRAIN:[loss=0.069]\n",
      "[71 epo]: TRAIN:[loss=0.068]\n",
      "[72 epo]: TRAIN:[loss=0.067]\n",
      "[73 epo]: TRAIN:[loss=0.066]\n",
      "[74 epo]: TRAIN:[loss=0.066]\n",
      "[75 epo]: TRAIN:[loss=0.065]\n",
      "[76 epo]: TRAIN:[loss=0.064]\n",
      "[77 epo]: TRAIN:[loss=0.064]\n",
      "[78 epo]: TRAIN:[loss=0.063]\n",
      "[79 epo]: TRAIN:[loss=0.063]\n",
      "[80 epo]: TRAIN:[loss=0.062]\n",
      "[81 epo]: TRAIN:[loss=0.061]\n",
      "[82 epo]: TRAIN:[loss=0.060]\n",
      "[83 epo]: TRAIN:[loss=0.060]\n",
      "[84 epo]: TRAIN:[loss=0.059]\n",
      "[85 epo]: TRAIN:[loss=0.058]\n",
      "[86 epo]: TRAIN:[loss=0.058]\n",
      "[87 epo]: TRAIN:[loss=0.057]\n",
      "[88 epo]: TRAIN:[loss=0.057]\n",
      "[89 epo]: TRAIN:[loss=0.056]\n",
      "[90 epo]: TRAIN:[loss=0.056]\n",
      "[91 epo]: TRAIN:[loss=0.055]\n",
      "[92 epo]: TRAIN:[loss=0.055]\n",
      "[93 epo]: TRAIN:[loss=0.054]\n",
      "[94 epo]: TRAIN:[loss=0.054]\n",
      "[95 epo]: TRAIN:[loss=0.053]\n",
      "[96 epo]: TRAIN:[loss=0.053]\n",
      "[97 epo]: TRAIN:[loss=0.052]\n",
      "[98 epo]: TRAIN:[loss=0.052]\n",
      "[99 epo]: TRAIN:[loss=0.051]\n",
      "[100 epo]: TRAIN:[loss=0.051]\n",
      "[101 epo]: TRAIN:[loss=0.050]\n",
      "[102 epo]: TRAIN:[loss=0.050]\n",
      "[103 epo]: TRAIN:[loss=0.050]\n",
      "[104 epo]: TRAIN:[loss=0.049]\n",
      "[105 epo]: TRAIN:[loss=0.049]\n",
      "[106 epo]: TRAIN:[loss=0.048]\n",
      "[107 epo]: TRAIN:[loss=0.048]\n",
      "[108 epo]: TRAIN:[loss=0.048]\n",
      "[109 epo]: TRAIN:[loss=0.047]\n",
      "[110 epo]: TRAIN:[loss=0.047]\n",
      "[111 epo]: TRAIN:[loss=0.047]\n",
      "[112 epo]: TRAIN:[loss=0.046]\n",
      "[113 epo]: TRAIN:[loss=0.046]\n",
      "[114 epo]: TRAIN:[loss=0.046]\n",
      "[115 epo]: TRAIN:[loss=0.045]\n",
      "[116 epo]: TRAIN:[loss=0.045]\n",
      "[117 epo]: TRAIN:[loss=0.045]\n",
      "[118 epo]: TRAIN:[loss=0.044]\n",
      "[119 epo]: TRAIN:[loss=0.044]\n",
      "[120 epo]: TRAIN:[loss=0.044]\n",
      "[121 epo]: TRAIN:[loss=0.043]\n",
      "[122 epo]: TRAIN:[loss=0.043]\n",
      "[123 epo]: TRAIN:[loss=0.043]\n",
      "[124 epo]: TRAIN:[loss=0.043]\n",
      "[125 epo]: TRAIN:[loss=0.042]\n",
      "[126 epo]: TRAIN:[loss=0.042]\n",
      "[127 epo]: TRAIN:[loss=0.042]\n",
      "[128 epo]: TRAIN:[loss=0.041]\n",
      "[129 epo]: TRAIN:[loss=0.041]\n",
      "[130 epo]: TRAIN:[loss=0.041]\n",
      "[131 epo]: TRAIN:[loss=0.041]\n",
      "[132 epo]: TRAIN:[loss=0.040]\n",
      "[133 epo]: TRAIN:[loss=0.040]\n",
      "[134 epo]: TRAIN:[loss=0.040]\n",
      "[135 epo]: TRAIN:[loss=0.040]\n",
      "[136 epo]: TRAIN:[loss=0.040]\n",
      "[137 epo]: TRAIN:[loss=0.039]\n",
      "[138 epo]: TRAIN:[loss=0.039]\n",
      "[139 epo]: TRAIN:[loss=0.039]\n",
      "[140 epo]: TRAIN:[loss=0.039]\n",
      "[141 epo]: TRAIN:[loss=0.038]\n",
      "[142 epo]: TRAIN:[loss=0.038]\n",
      "[143 epo]: TRAIN:[loss=0.038]\n",
      "[144 epo]: TRAIN:[loss=0.038]\n",
      "[145 epo]: TRAIN:[loss=0.038]\n",
      "[146 epo]: TRAIN:[loss=0.038]\n",
      "[147 epo]: TRAIN:[loss=0.037]\n",
      "[148 epo]: TRAIN:[loss=0.037]\n",
      "[149 epo]: TRAIN:[loss=0.037]\n",
      "[150 epo]: TRAIN:[loss=0.037]\n",
      "[151 epo]: TRAIN:[loss=0.037]\n",
      "[152 epo]: TRAIN:[loss=0.036]\n",
      "[153 epo]: TRAIN:[loss=0.036]\n",
      "[154 epo]: TRAIN:[loss=0.036]\n",
      "[155 epo]: TRAIN:[loss=0.036]\n",
      "[156 epo]: TRAIN:[loss=0.036]\n",
      "[157 epo]: TRAIN:[loss=0.036]\n",
      "[158 epo]: TRAIN:[loss=0.036]\n",
      "[159 epo]: TRAIN:[loss=0.035]\n",
      "[160 epo]: TRAIN:[loss=0.035]\n",
      "[161 epo]: TRAIN:[loss=0.035]\n",
      "[162 epo]: TRAIN:[loss=0.035]\n",
      "[163 epo]: TRAIN:[loss=0.035]\n",
      "[164 epo]: TRAIN:[loss=0.035]\n",
      "[165 epo]: TRAIN:[loss=0.035]\n",
      "[166 epo]: TRAIN:[loss=0.034]\n",
      "[167 epo]: TRAIN:[loss=0.034]\n",
      "[168 epo]: TRAIN:[loss=0.034]\n",
      "[169 epo]: TRAIN:[loss=0.034]\n",
      "[170 epo]: TRAIN:[loss=0.034]\n",
      "[171 epo]: TRAIN:[loss=0.034]\n",
      "[172 epo]: TRAIN:[loss=0.034]\n",
      "[173 epo]: TRAIN:[loss=0.034]\n",
      "[174 epo]: TRAIN:[loss=0.033]\n",
      "[175 epo]: TRAIN:[loss=0.033]\n",
      "[176 epo]: TRAIN:[loss=0.033]\n",
      "[177 epo]: TRAIN:[loss=0.033]\n",
      "[178 epo]: TRAIN:[loss=0.033]\n",
      "[179 epo]: TRAIN:[loss=0.033]\n",
      "[180 epo]: TRAIN:[loss=0.033]\n",
      "[181 epo]: TRAIN:[loss=0.033]\n",
      "[182 epo]: TRAIN:[loss=0.033]\n",
      "[183 epo]: TRAIN:[loss=0.032]\n",
      "[184 epo]: TRAIN:[loss=0.032]\n",
      "[185 epo]: TRAIN:[loss=0.032]\n",
      "[186 epo]: TRAIN:[loss=0.032]\n",
      "[187 epo]: TRAIN:[loss=0.032]\n",
      "[188 epo]: TRAIN:[loss=0.032]\n",
      "[189 epo]: TRAIN:[loss=0.032]\n",
      "[190 epo]: TRAIN:[loss=0.032]\n",
      "[191 epo]: TRAIN:[loss=0.032]\n",
      "[192 epo]: TRAIN:[loss=0.032]\n",
      "[193 epo]: TRAIN:[loss=0.032]\n",
      "[194 epo]: TRAIN:[loss=0.031]\n",
      "[195 epo]: TRAIN:[loss=0.031]\n",
      "[196 epo]: TRAIN:[loss=0.031]\n",
      "[197 epo]: TRAIN:[loss=0.031]\n",
      "[198 epo]: TRAIN:[loss=0.031]\n",
      "[199 epo]: TRAIN:[loss=0.031]\n",
      "[200 epo]: TRAIN:[loss=0.031]\n",
      "[201 epo]: TRAIN:[loss=0.031]\n",
      "[202 epo]: TRAIN:[loss=0.031]\n",
      "[203 epo]: TRAIN:[loss=0.031]\n",
      "[204 epo]: TRAIN:[loss=0.031]\n",
      "[205 epo]: TRAIN:[loss=0.031]\n",
      "[206 epo]: TRAIN:[loss=0.031]\n",
      "[207 epo]: TRAIN:[loss=0.030]\n",
      "[208 epo]: TRAIN:[loss=0.030]\n",
      "[209 epo]: TRAIN:[loss=0.030]\n",
      "[210 epo]: TRAIN:[loss=0.030]\n",
      "[211 epo]: TRAIN:[loss=0.030]\n",
      "[212 epo]: TRAIN:[loss=0.030]\n",
      "[213 epo]: TRAIN:[loss=0.030]\n",
      "[214 epo]: TRAIN:[loss=0.030]\n",
      "[215 epo]: TRAIN:[loss=0.030]\n",
      "[216 epo]: TRAIN:[loss=0.030]\n",
      "[217 epo]: TRAIN:[loss=0.030]\n",
      "[218 epo]: TRAIN:[loss=0.030]\n",
      "[219 epo]: TRAIN:[loss=0.030]\n",
      "[220 epo]: TRAIN:[loss=0.030]\n",
      "[221 epo]: TRAIN:[loss=0.030]\n",
      "[222 epo]: TRAIN:[loss=0.030]\n",
      "[223 epo]: TRAIN:[loss=0.030]\n",
      "[224 epo]: TRAIN:[loss=0.029]\n",
      "[225 epo]: TRAIN:[loss=0.029]\n",
      "[226 epo]: TRAIN:[loss=0.029]\n",
      "[227 epo]: TRAIN:[loss=0.029]\n",
      "[228 epo]: TRAIN:[loss=0.029]\n",
      "[229 epo]: TRAIN:[loss=0.029]\n",
      "[230 epo]: TRAIN:[loss=0.029]\n",
      "[231 epo]: TRAIN:[loss=0.029]\n",
      "[232 epo]: TRAIN:[loss=0.029]\n",
      "[233 epo]: TRAIN:[loss=0.029]\n",
      "[234 epo]: TRAIN:[loss=0.029]\n",
      "[235 epo]: TRAIN:[loss=0.029]\n",
      "[236 epo]: TRAIN:[loss=0.029]\n",
      "[237 epo]: TRAIN:[loss=0.029]\n",
      "[238 epo]: TRAIN:[loss=0.029]\n",
      "[239 epo]: TRAIN:[loss=0.029]\n",
      "[240 epo]: TRAIN:[loss=0.029]\n",
      "[241 epo]: TRAIN:[loss=0.029]\n",
      "[242 epo]: TRAIN:[loss=0.029]\n",
      "[243 epo]: TRAIN:[loss=0.029]\n",
      "[244 epo]: TRAIN:[loss=0.029]\n",
      "[245 epo]: TRAIN:[loss=0.028]\n",
      "[246 epo]: TRAIN:[loss=0.028]\n",
      "[247 epo]: TRAIN:[loss=0.028]\n",
      "[248 epo]: TRAIN:[loss=0.028]\n",
      "[249 epo]: TRAIN:[loss=0.028]\n",
      "[250 epo]: TRAIN:[loss=0.028]\n",
      "[251 epo]: TRAIN:[loss=0.028]\n",
      "[252 epo]: TRAIN:[loss=0.028]\n",
      "[253 epo]: TRAIN:[loss=0.028]\n",
      "[254 epo]: TRAIN:[loss=0.028]\n",
      "[255 epo]: TRAIN:[loss=0.028]\n",
      "[256 epo]: TRAIN:[loss=0.028]\n",
      "[257 epo]: TRAIN:[loss=0.028]\n",
      "[258 epo]: TRAIN:[loss=0.028]\n",
      "[259 epo]: TRAIN:[loss=0.028]\n",
      "[260 epo]: TRAIN:[loss=0.028]\n",
      "[261 epo]: TRAIN:[loss=0.028]\n",
      "[262 epo]: TRAIN:[loss=0.028]\n",
      "[263 epo]: TRAIN:[loss=0.028]\n",
      "[264 epo]: TRAIN:[loss=0.028]\n",
      "[265 epo]: TRAIN:[loss=0.028]\n",
      "[266 epo]: TRAIN:[loss=0.028]\n",
      "[267 epo]: TRAIN:[loss=0.028]\n",
      "[268 epo]: TRAIN:[loss=0.028]\n",
      "[269 epo]: TRAIN:[loss=0.028]\n",
      "[270 epo]: TRAIN:[loss=0.028]\n",
      "[271 epo]: TRAIN:[loss=0.028]\n",
      "[272 epo]: TRAIN:[loss=0.028]\n",
      "[273 epo]: TRAIN:[loss=0.028]\n",
      "[274 epo]: TRAIN:[loss=0.028]\n",
      "[275 epo]: TRAIN:[loss=0.027]\n",
      "[276 epo]: TRAIN:[loss=0.027]\n",
      "[277 epo]: TRAIN:[loss=0.027]\n",
      "[278 epo]: TRAIN:[loss=0.027]\n",
      "[279 epo]: TRAIN:[loss=0.027]\n",
      "[280 epo]: TRAIN:[loss=0.027]\n",
      "[281 epo]: TRAIN:[loss=0.027]\n",
      "[282 epo]: TRAIN:[loss=0.027]\n",
      "[283 epo]: TRAIN:[loss=0.027]\n",
      "[284 epo]: TRAIN:[loss=0.027]\n",
      "[285 epo]: TRAIN:[loss=0.027]\n",
      "[286 epo]: TRAIN:[loss=0.027]\n",
      "[287 epo]: TRAIN:[loss=0.027]\n",
      "[288 epo]: TRAIN:[loss=0.027]\n",
      "[289 epo]: TRAIN:[loss=0.027]\n",
      "[290 epo]: TRAIN:[loss=0.027]\n",
      "[291 epo]: TRAIN:[loss=0.027]\n",
      "[292 epo]: TRAIN:[loss=0.027]\n",
      "[293 epo]: TRAIN:[loss=0.027]\n",
      "[294 epo]: TRAIN:[loss=0.027]\n",
      "[295 epo]: TRAIN:[loss=0.027]\n",
      "[296 epo]: TRAIN:[loss=0.027]\n",
      "[297 epo]: TRAIN:[loss=0.027]\n",
      "[298 epo]: TRAIN:[loss=0.027]\n",
      "[299 epo]: TRAIN:[loss=0.027]\n",
      "[300 epo]: TRAIN:[loss=0.027]\n",
      "[301 epo]: TRAIN:[loss=0.027]\n",
      "[302 epo]: TRAIN:[loss=0.027]\n",
      "[303 epo]: TRAIN:[loss=0.027]\n",
      "[304 epo]: TRAIN:[loss=0.027]\n",
      "[305 epo]: TRAIN:[loss=0.027]\n",
      "[306 epo]: TRAIN:[loss=0.027]\n",
      "[307 epo]: TRAIN:[loss=0.027]\n",
      "[308 epo]: TRAIN:[loss=0.027]\n",
      "[309 epo]: TRAIN:[loss=0.027]\n",
      "[310 epo]: TRAIN:[loss=0.027]\n",
      "[311 epo]: TRAIN:[loss=0.027]\n",
      "[312 epo]: TRAIN:[loss=0.027]\n",
      "[313 epo]: TRAIN:[loss=0.027]\n",
      "[314 epo]: TRAIN:[loss=0.027]\n",
      "[315 epo]: TRAIN:[loss=0.027]\n",
      "[316 epo]: TRAIN:[loss=0.027]\n",
      "[317 epo]: TRAIN:[loss=0.027]\n",
      "[318 epo]: TRAIN:[loss=0.027]\n",
      "[319 epo]: TRAIN:[loss=0.027]\n",
      "[320 epo]: TRAIN:[loss=0.027]\n",
      "[321 epo]: TRAIN:[loss=0.027]\n",
      "[322 epo]: TRAIN:[loss=0.026]\n",
      "[323 epo]: TRAIN:[loss=0.026]\n",
      "[324 epo]: TRAIN:[loss=0.026]\n",
      "[325 epo]: TRAIN:[loss=0.026]\n",
      "[326 epo]: TRAIN:[loss=0.026]\n",
      "[327 epo]: TRAIN:[loss=0.026]\n",
      "[328 epo]: TRAIN:[loss=0.026]\n",
      "[329 epo]: TRAIN:[loss=0.026]\n",
      "[330 epo]: TRAIN:[loss=0.026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[331 epo]: TRAIN:[loss=0.026]\n",
      "[332 epo]: TRAIN:[loss=0.026]\n",
      "[333 epo]: TRAIN:[loss=0.026]\n",
      "[334 epo]: TRAIN:[loss=0.026]\n",
      "[335 epo]: TRAIN:[loss=0.026]\n",
      "[336 epo]: TRAIN:[loss=0.026]\n",
      "[337 epo]: TRAIN:[loss=0.026]\n",
      "[338 epo]: TRAIN:[loss=0.026]\n",
      "[339 epo]: TRAIN:[loss=0.026]\n",
      "[340 epo]: TRAIN:[loss=0.026]\n",
      "[341 epo]: TRAIN:[loss=0.026]\n",
      "[342 epo]: TRAIN:[loss=0.026]\n",
      "[343 epo]: TRAIN:[loss=0.026]\n",
      "[344 epo]: TRAIN:[loss=0.026]\n",
      "[345 epo]: TRAIN:[loss=0.026]\n",
      "[346 epo]: TRAIN:[loss=0.026]\n",
      "[347 epo]: TRAIN:[loss=0.026]\n",
      "[348 epo]: TRAIN:[loss=0.026]\n",
      "[349 epo]: TRAIN:[loss=0.026]\n",
      "[350 epo]: TRAIN:[loss=0.026]\n",
      "[351 epo]: TRAIN:[loss=0.026]\n",
      "[352 epo]: TRAIN:[loss=0.026]\n",
      "[353 epo]: TRAIN:[loss=0.026]\n",
      "[354 epo]: TRAIN:[loss=0.026]\n",
      "[355 epo]: TRAIN:[loss=0.026]\n",
      "[356 epo]: TRAIN:[loss=0.026]\n",
      "[357 epo]: TRAIN:[loss=0.026]\n",
      "[358 epo]: TRAIN:[loss=0.026]\n",
      "[359 epo]: TRAIN:[loss=0.026]\n",
      "[360 epo]: TRAIN:[loss=0.026]\n",
      "[361 epo]: TRAIN:[loss=0.026]\n",
      "[362 epo]: TRAIN:[loss=0.026]\n",
      "[363 epo]: TRAIN:[loss=0.026]\n",
      "[364 epo]: TRAIN:[loss=0.026]\n",
      "[365 epo]: TRAIN:[loss=0.026]\n",
      "[366 epo]: TRAIN:[loss=0.026]\n",
      "[367 epo]: TRAIN:[loss=0.026]\n",
      "[368 epo]: TRAIN:[loss=0.026]\n",
      "[369 epo]: TRAIN:[loss=0.026]\n",
      "[370 epo]: TRAIN:[loss=0.026]\n",
      "[371 epo]: TRAIN:[loss=0.026]\n",
      "[372 epo]: TRAIN:[loss=0.026]\n",
      "[373 epo]: TRAIN:[loss=0.026]\n",
      "[374 epo]: TRAIN:[loss=0.026]\n",
      "[375 epo]: TRAIN:[loss=0.026]\n",
      "[376 epo]: TRAIN:[loss=0.026]\n",
      "[377 epo]: TRAIN:[loss=0.026]\n",
      "[378 epo]: TRAIN:[loss=0.026]\n",
      "[379 epo]: TRAIN:[loss=0.026]\n",
      "[380 epo]: TRAIN:[loss=0.026]\n",
      "[381 epo]: TRAIN:[loss=0.026]\n",
      "[382 epo]: TRAIN:[loss=0.026]\n",
      "[383 epo]: TRAIN:[loss=0.026]\n",
      "[384 epo]: TRAIN:[loss=0.026]\n",
      "[385 epo]: TRAIN:[loss=0.026]\n",
      "[386 epo]: TRAIN:[loss=0.026]\n",
      "[387 epo]: TRAIN:[loss=0.026]\n",
      "[388 epo]: TRAIN:[loss=0.026]\n",
      "[389 epo]: TRAIN:[loss=0.026]\n",
      "[390 epo]: TRAIN:[loss=0.026]\n",
      "[391 epo]: TRAIN:[loss=0.026]\n",
      "[392 epo]: TRAIN:[loss=0.026]\n",
      "[393 epo]: TRAIN:[loss=0.026]\n",
      "[394 epo]: TRAIN:[loss=0.026]\n",
      "[395 epo]: TRAIN:[loss=0.026]\n",
      "[396 epo]: TRAIN:[loss=0.026]\n",
      "[397 epo]: TRAIN:[loss=0.026]\n",
      "[398 epo]: TRAIN:[loss=0.026]\n",
      "[399 epo]: TRAIN:[loss=0.026]\n",
      "[400 epo]: TRAIN:[loss=0.026]\n",
      "[401 epo]: TRAIN:[loss=0.026]\n",
      "[402 epo]: TRAIN:[loss=0.026]\n",
      "[403 epo]: TRAIN:[loss=0.026]\n",
      "[404 epo]: TRAIN:[loss=0.026]\n",
      "[405 epo]: TRAIN:[loss=0.026]\n",
      "[406 epo]: TRAIN:[loss=0.026]\n",
      "[407 epo]: TRAIN:[loss=0.026]\n",
      "[408 epo]: TRAIN:[loss=0.026]\n",
      "[409 epo]: TRAIN:[loss=0.026]\n",
      "[410 epo]: TRAIN:[loss=0.026]\n",
      "[411 epo]: TRAIN:[loss=0.026]\n",
      "[412 epo]: TRAIN:[loss=0.026]\n",
      "[413 epo]: TRAIN:[loss=0.026]\n",
      "[414 epo]: TRAIN:[loss=0.025]\n",
      "[415 epo]: TRAIN:[loss=0.025]\n",
      "[416 epo]: TRAIN:[loss=0.025]\n",
      "[417 epo]: TRAIN:[loss=0.025]\n",
      "[418 epo]: TRAIN:[loss=0.025]\n",
      "[419 epo]: TRAIN:[loss=0.025]\n",
      "[420 epo]: TRAIN:[loss=0.025]\n",
      "[421 epo]: TRAIN:[loss=0.025]\n",
      "[422 epo]: TRAIN:[loss=0.025]\n",
      "[423 epo]: TRAIN:[loss=0.025]\n",
      "[424 epo]: TRAIN:[loss=0.025]\n",
      "[425 epo]: TRAIN:[loss=0.025]\n",
      "[426 epo]: TRAIN:[loss=0.025]\n",
      "[427 epo]: TRAIN:[loss=0.025]\n",
      "[428 epo]: TRAIN:[loss=0.025]\n",
      "[429 epo]: TRAIN:[loss=0.025]\n",
      "[430 epo]: TRAIN:[loss=0.025]\n",
      "[431 epo]: TRAIN:[loss=0.025]\n",
      "[432 epo]: TRAIN:[loss=0.025]\n",
      "[433 epo]: TRAIN:[loss=0.025]\n",
      "[434 epo]: TRAIN:[loss=0.025]\n",
      "[435 epo]: TRAIN:[loss=0.025]\n",
      "[436 epo]: TRAIN:[loss=0.025]\n",
      "[437 epo]: TRAIN:[loss=0.025]\n",
      "[438 epo]: TRAIN:[loss=0.025]\n",
      "[439 epo]: TRAIN:[loss=0.025]\n",
      "[440 epo]: TRAIN:[loss=0.025]\n",
      "[441 epo]: TRAIN:[loss=0.025]\n",
      "[442 epo]: TRAIN:[loss=0.025]\n",
      "[443 epo]: TRAIN:[loss=0.025]\n",
      "[444 epo]: TRAIN:[loss=0.025]\n",
      "[445 epo]: TRAIN:[loss=0.025]\n",
      "[446 epo]: TRAIN:[loss=0.025]\n",
      "[447 epo]: TRAIN:[loss=0.025]\n",
      "[448 epo]: TRAIN:[loss=0.025]\n",
      "[449 epo]: TRAIN:[loss=0.025]\n",
      "[450 epo]: TRAIN:[loss=0.025]\n",
      "[451 epo]: TRAIN:[loss=0.025]\n",
      "[452 epo]: TRAIN:[loss=0.025]\n",
      "[453 epo]: TRAIN:[loss=0.025]\n",
      "[454 epo]: TRAIN:[loss=0.025]\n",
      "[455 epo]: TRAIN:[loss=0.025]\n",
      "[456 epo]: TRAIN:[loss=0.025]\n",
      "[457 epo]: TRAIN:[loss=0.025]\n",
      "[458 epo]: TRAIN:[loss=0.025]\n",
      "[459 epo]: TRAIN:[loss=0.025]\n",
      "[460 epo]: TRAIN:[loss=0.025]\n",
      "[461 epo]: TRAIN:[loss=0.025]\n",
      "[462 epo]: TRAIN:[loss=0.025]\n",
      "[463 epo]: TRAIN:[loss=0.025]\n",
      "[464 epo]: TRAIN:[loss=0.025]\n",
      "[465 epo]: TRAIN:[loss=0.025]\n",
      "[466 epo]: TRAIN:[loss=0.025]\n",
      "[467 epo]: TRAIN:[loss=0.025]\n",
      "[468 epo]: TRAIN:[loss=0.025]\n",
      "[469 epo]: TRAIN:[loss=0.025]\n",
      "[470 epo]: TRAIN:[loss=0.025]\n",
      "[471 epo]: TRAIN:[loss=0.025]\n",
      "[472 epo]: TRAIN:[loss=0.025]\n",
      "[473 epo]: TRAIN:[loss=0.025]\n",
      "[474 epo]: TRAIN:[loss=0.025]\n",
      "[475 epo]: TRAIN:[loss=0.025]\n",
      "[476 epo]: TRAIN:[loss=0.025]\n",
      "[477 epo]: TRAIN:[loss=0.025]\n",
      "[478 epo]: TRAIN:[loss=0.025]\n",
      "[479 epo]: TRAIN:[loss=0.025]\n",
      "[480 epo]: TRAIN:[loss=0.025]\n",
      "[481 epo]: TRAIN:[loss=0.025]\n",
      "[482 epo]: TRAIN:[loss=0.025]\n",
      "[483 epo]: TRAIN:[loss=0.025]\n",
      "[484 epo]: TRAIN:[loss=0.025]\n",
      "[485 epo]: TRAIN:[loss=0.025]\n",
      "[486 epo]: TRAIN:[loss=0.025]\n",
      "[487 epo]: TRAIN:[loss=0.025]\n",
      "[488 epo]: TRAIN:[loss=0.025]\n",
      "[489 epo]: TRAIN:[loss=0.025]\n",
      "[490 epo]: TRAIN:[loss=0.025]\n",
      "[491 epo]: TRAIN:[loss=0.025]\n",
      "[492 epo]: TRAIN:[loss=0.025]\n",
      "[493 epo]: TRAIN:[loss=0.025]\n",
      "[494 epo]: TRAIN:[loss=0.025]\n",
      "[495 epo]: TRAIN:[loss=0.025]\n",
      "[496 epo]: TRAIN:[loss=0.025]\n",
      "[497 epo]: TRAIN:[loss=0.025]\n",
      "[498 epo]: TRAIN:[loss=0.025]\n",
      "[499 epo]: TRAIN:[loss=0.025]\n",
      "[500 epo]: TRAIN:[loss=0.025]\n",
      "Finished Training\n",
      "CPU times: user 522 ms, sys: 138 ms, total: 660 ms\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "from metal.label_model import LabelModel\n",
    "\n",
    "gen_model = LabelModel(k = 2)\n",
    "#%time gen_model.train_model(L_train[0],n_epochs=500, print_every=100)\n",
    "%time gen_model.train_model(L_train[0], class_balance = [0.8,0.2],n_epochs=500, print_every=100)\n",
    "\n",
    "train_marginals = gen_model.predict_proba(L_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_marginals[:, 1], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_funcs_train_set = {}\n",
    "train_set_LFs = L_train[0].toarray()\n",
    "if pickled: label_names = [i for i in range(L_train[0].shape[1])]\n",
    "else: label_names = labeler.get_keys()\n",
    "for i in range(train_set_LFs.shape[1]):\n",
    "    label_funcs_train_set[label_names[i]] = train_set_LFs[:,i]\n",
    "    \n",
    "period = []\n",
    "revenue = []\n",
    "doc = []\n",
    "table = []\n",
    "\n",
    "for i in range(len(train_cands[0])):\n",
    "    period.append(train_cands[0][i].period.context.get_span())\n",
    "    revenue.append(train_cands[0][i].revenue.context.get_span())\n",
    "    doc.append(train_cands[0][i].revenue.context.sentence.document)\n",
    "    table.append(train_cands[0][i].revenue.context.sentence.table.position)\n",
    "\n",
    "results_train = np.c_[period, revenue, doc, table, train_marginals[:,1]]\n",
    "results_train = pd.DataFrame(results_train)\n",
    "results_train.columns=['period', 'revenue', 'doc', 'table', 'probability']\n",
    "for lf, labeling in label_funcs_train_set.items():\n",
    "    results_train[lf] = labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83928289 0.16071711]\n",
      " [0.54479539 0.45520461]\n",
      " [0.83928289 0.16071711]\n",
      " ...\n",
      " [0.07236924 0.92763076]\n",
      " [0.07236924 0.92763076]\n",
      " [0.07236924 0.92763076]]\n"
     ]
    }
   ],
   "source": [
    "train_marginals_custom, results_train_2 = gen_custom_marginals(results_train)\n",
    "print(train_marginals_custom)\n",
    "#results_train_2.to_csv('lf_analysis_train_set2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(train_marginals_custom[:, 1], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals_custom_df = pd.DataFrame(train_marginals_custom)\n",
    "train_marginals_custom_df.to_csv('train_margs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-09 20:56:32,592][INFO] fonduer.learning.classifier:117 - Loading default parameters for Logistic Regression\n",
      "[2019-06-09 20:56:45,627][INFO] fonduer.learning.classifier:213 - Using GPU...\n",
      "[2019-06-09 20:56:45,627][INFO] fonduer.learning.classifier:215 - Settings: {'log_dir': 'logs/2019-06-09_18-19-19/2019-06-09_20-56-32_NeuralNet', 'n_epochs': 1, 'lr': 1e-05, 'batch_size': 64, 'shuffle': True, 'seed': 1234, 'host_device': 'GPU', 'bias': False, 'input_dim': 110096}\n",
      "[2019-06-09 20:56:45,648][INFO] fonduer.learning.classifier:233 - [NeuralNet] Training model\n",
      "[2019-06-09 20:56:45,649][INFO] fonduer.learning.classifier:235 - [NeuralNet] n_train=12948 #epochs=1 batch size=64\n",
      "[2019-06-09 21:00:05,215][INFO] fonduer.learning.classifier:289 - [NeuralNet] Epoch 1 (199.57s)\tAverage loss=0.705992\n",
      "[2019-06-09 21:00:05,350][INFO] fonduer.learning.classifier:516 - [NeuralNet] Model saved as checkpoint_epoch_1.pt in logs/2019-06-09_18-19-19/2019-06-09_20-56-32_NeuralNet\n",
      "[2019-06-09 21:00:05,350][INFO] fonduer.learning.classifier:317 - Saving final model as best checkpoint logs/2019-06-09_18-19-19/2019-06-09_20-56-32_NeuralNet/checkpoint_epoch_1.pt.\n",
      "[2019-06-09 21:00:05,445][INFO] fonduer.learning.classifier:327 - [NeuralNet] Training done (199.80s)\n",
      "[2019-06-09 21:00:05,446][INFO] fonduer.learning.classifier:330 - Loading best checkpoint\n",
      "[2019-06-09 21:00:05,510][INFO] fonduer.learning.classifier:546 - [NeuralNet] Model loaded as best_model.pt in logs/2019-06-09_18-19-19/2019-06-09_20-56-32_NeuralNet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 25.9 s, total: 3min 34s\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%aimport fonduer.learning\n",
    "%aimport fonduer.utils\n",
    "%autoreload 1\n",
    "\n",
    "import torch\n",
    "import fonduer.learning\n",
    "from fonduer.learning import LogisticRegression\n",
    "from fonduer.learning import LSTM\n",
    "from fonduer.learning import NeuralNet\n",
    "\n",
    "torch.set_num_threads(PARALLEL)\n",
    "print(torch.get_num_threads())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#neural = NeuralNet(structure = [200, 20], F = F_train[0])\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "#  neural = nn.DataParallel(neural)\n",
    "\n",
    "#model.to(device)\n",
    "\n",
    "neural2 = NeuralNet(structure = [200, 20], F = F_train[0])\n",
    "%time neural2.train((train_cands[0], F_train[0]), train_marginals, n_epochs = 1, batch_size = 64, lr = 0.00001, print_freq = 1, host_device = \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model to Iterate on Labeling Functions on the dev/train set¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pickle_in = open(\"pickle.L_Dev_full\", \"rb\")\n",
    "    L_dev_dense = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(\"pickle.L_Test_full\", \"rb\")\n",
    "    L_test_dense = pickle.load(pickle_in)\n",
    "\n",
    "    L_dev = [sparse.csr_matrix(L_dev_dense)]\n",
    "    L_test = [sparse.csr_matrix(L_test_dense)]\n",
    "    \n",
    "except:\n",
    "    \n",
    "    labeler.apply(split=1, lfs=[LFs])\n",
    "    %time L_dev = labeler.get_label_matrices(dev_cands)\n",
    "\n",
    "    labeler.apply(split=2, lfs=[LFs])\n",
    "    %time L_test = labeler.get_label_matrices(test_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labeler.apply(split=1, lfs=[LFs])\n",
    "%time L_dev = labeler.get_label_matrices(dev_cands)\n",
    "\n",
    "labeler.apply(split=2, lfs=[LFs])\n",
    "%time L_test = labeler.get_label_matrices(test_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_funcs_dev_set = {}\n",
    "dev_set_LFs = L_dev[0].toarray()\n",
    "#label_names = labeler.get_keys()\n",
    "label_names = [i for i in range(L_dev[0].shape[1])]\n",
    "for i in range(dev_set_LFs.shape[1]):\n",
    "    label_funcs_dev_set[label_names[i]] = dev_set_LFs[:,i]\n",
    "    \n",
    "label_funcs_test_set = {}\n",
    "test_set_LFs = L_test[0].toarray()\n",
    "#label_names = labeler.get_keys()\n",
    "label_names = [i for i in range(L_test[0].shape[1])]\n",
    "for i in range(test_set_LFs.shape[1]):\n",
    "    label_funcs_test_set[label_names[i]] = test_set_LFs[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-09 19:50:30,792][INFO] fonduer.learning.classifier:403 - Using positive label class 2 with threshold 0.4\n",
      "[2019-06-09 19:50:55,483][INFO] fonduer.learning.classifier:403 - Using positive label class 2 with threshold 0.4\n"
     ]
    }
   ],
   "source": [
    "#testing dev and test set results\n",
    "dev_score = neural2.predict((dev_cands[0], F_dev[0]), b = 0.4, pos_label = TRUE, return_probs=True)\n",
    "test_score = neural2.predict((test_cands[0], F_test[0]), b = 0.4, pos_label = TRUE, return_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev positive count: 1248\n",
      "test positive count: 942\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#dev_set numbers\n",
    "count = 0\n",
    "for i in dev_score[0]:\n",
    "    if i == 2:\n",
    "        count += 1\n",
    "print('dev positive count: ' + str(count))\n",
    "\n",
    "#test_set numbers\n",
    "count = 0\n",
    "for i in test_score[0]:\n",
    "    if i == 2:\n",
    "        count += 1\n",
    "print('test positive count: ' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2529"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_cands[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# updating results_train\n",
    "results_train.rename(columns={0:'temp'}, inplace=True)\n",
    "results_train = extract_cik(results_train)\n",
    "results_train.rename(columns={'temp':0}, inplace=True)\n",
    "\n",
    "#pending add albeling functions results here\n",
    "\n",
    "period = []\n",
    "revenue = []\n",
    "doc = []\n",
    "table = []\n",
    "\n",
    "for i in range(len(dev_cands[0])):\n",
    "    period.append(dev_cands[0][i].period.context.get_span())\n",
    "    revenue.append(dev_cands[0][i].revenue.context.get_span())\n",
    "    doc.append(dev_cands[0][i].revenue.context.sentence.document)\n",
    "    table.append(dev_cands[0][i].revenue.context.sentence.table.position)\n",
    "\n",
    "results_dev = np.c_[period, revenue, doc, table, dev_score[0], dev_score[1][:,1]]\n",
    "results_dev = pd.DataFrame(results_dev)\n",
    "results_dev.columns=['period', 'revenue', 'doc', 'table', 'dev_score', 'probability']\n",
    "for lf, labeling in label_funcs_dev_set.items():\n",
    "    results_dev[lf] = labeling\n",
    "results_dev.rename(columns={0:'temp'}, inplace=True)\n",
    "results_dev = extract_cik(results_dev)\n",
    "results_dev.rename(columns={'temp':0}, inplace=True)\n",
    "    \n",
    "period = []\n",
    "revenue = []\n",
    "doc = []\n",
    "table = []\n",
    "\n",
    "for i in range(len(test_cands[0])):\n",
    "    period.append(test_cands[0][i].period.context.get_span())\n",
    "    revenue.append(test_cands[0][i].revenue.context.get_span())\n",
    "    doc.append(test_cands[0][i].revenue.context.sentence.document)\n",
    "    table.append(test_cands[0][i].revenue.context.sentence.table.position)\n",
    "\n",
    "results_test = np.c_[period, revenue, doc, table, test_score[0], test_score[1][:,1]]\n",
    "results_test = pd.DataFrame(results_test)\n",
    "results_test.columns=['period', 'revenue', 'doc', 'table', 'test_score', 'probability']\n",
    "for lf, labeling in label_funcs_test_set.items():\n",
    "    results_test[lf] = labeling\n",
    "results_test.rename(columns={0:'temp'}, inplace=True)\n",
    "results_test = extract_cik(results_test)\n",
    "results_test.rename(columns={'temp':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train.to_csv('results_train0.csv')\n",
    "results_dev.to_csv('results_dev0.csv')\n",
    "results_test.to_csv('results_test0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_results_path = \"/home/ubuntu/anaconda3/results_dev0.csv\"\n",
    "results_dev2 = pd.read_csv(y_dev_results_path)\n",
    "#results_dev2 = results_dev\n",
    "del results_dev2['Unnamed: 0']\n",
    "results_dev2['revenue_string'] = results_dev2['revenue'].astype(str)\n",
    "\n",
    "y_dev_path = \"/home/ubuntu/anaconda3/Y_final.csv\"\n",
    "y_dev = pd.read_csv(y_dev_path)\n",
    "y_dev['Revenue_string'] = y_dev['Revenue'].astype(str)\n",
    "y_dev['True_label'] = 2\n",
    "del y_dev['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_dev = results_dev2.merge(y_dev, left_on=['CIK','revenue_string'], right_on=['CIK','Revenue_string'], how='left')\n",
    "Results_dev = Results_dev[['period', 'revenue', 'doc', 'table', 'probability', 'True_label']]\n",
    "Results_dev = Results_dev.fillna(1)\n",
    "\n",
    "Results_dev = Results_dev.sort_values(['doc', 'probability'], ascending=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aux = []\n",
    "Aux.append(1)\n",
    "dev_score = []\n",
    "\n",
    "for i in range(1, len(Results_dev)):\n",
    "    if Results_dev['doc'][i] == Results_dev['doc'][i-1]:\n",
    "        Aux.append(Aux[i-1]+1)\n",
    "    else:\n",
    "        Aux.append(1)\n",
    "\n",
    "for i in range(1, len(Aux)):\n",
    "    if Aux[i] in range(1, 10):\n",
    "        dev_score.append(2)\n",
    "    else:\n",
    "        dev_score.append(1)\n",
    "dev_score.append(1)\n",
    "\n",
    "dev_score = pd.DataFrame(dev_score)        \n",
    "Results_dev = pd.concat([Results_dev, dev_score], axis=1)\n",
    "Results_dev.rename(columns={0:'dev_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = []\n",
    "TN = []\n",
    "FP = []\n",
    "FN = []\n",
    "\n",
    "for i in range(len(Results_dev)):\n",
    "    if Results_dev['True_label'][i] == 2:\n",
    "        if Results_dev['dev_score'][i] == 2:\n",
    "            TP.append(1)\n",
    "            TN.append(0)\n",
    "            FP.append(0)\n",
    "            FN.append(0)\n",
    "        else:\n",
    "            TP.append(0)\n",
    "            TN.append(0)\n",
    "            FP.append(0)\n",
    "            FN.append(1)\n",
    "    else:\n",
    "        if Results_dev['dev_score'][i] == 2:\n",
    "            TP.append(0)\n",
    "            TN.append(0)\n",
    "            FP.append(1)\n",
    "            FN.append(0)\n",
    "        else:\n",
    "            TP.append(0)\n",
    "            TN.append(1)\n",
    "            FP.append(0)\n",
    "            FN.append(0)\n",
    "\n",
    "TP = pd.DataFrame(TP)\n",
    "TN = pd.DataFrame(TN)\n",
    "FP = pd.DataFrame(FP)\n",
    "FN = pd.DataFrame(FN)\n",
    "Results_dev = pd.concat([Results_dev, TP, TN, FP, FN], axis=1)\n",
    "Results_dev.columns = ['period','revenue','doc','table','probability','True_label','dev_score','TP','TN','FP','FN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in the development set is 31.9%\n",
      "The precision in the development set is 24.1%\n",
      "The recall in the development set is 93.9%\n",
      "The F1 score in the development set is 38.4%\n"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "TP = Results_dev['TP'].sum()\n",
    "TN = Results_dev['TN'].sum()\n",
    "FP = Results_dev['FP'].sum()\n",
    "FN = Results_dev['FN'].sum()\n",
    "\n",
    "Accuracy = (TP+TN)/len(Results_dev)\n",
    "Precision = TP / (TP+FP)\n",
    "Recall = TP / (TP+FN)\n",
    "F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "print(\"The accuracy in the development set is %.1f%%\" % (100*Accuracy))\n",
    "print(\"The precision in the development set is %.1f%%\" % (100*Precision))\n",
    "print(\"The recall in the development set is %.1f%%\" % (100*Recall))\n",
    "print(\"The F1 score in the development set is %.1f%%\" % (100*F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_results_path = \"/home/ubuntu/anaconda3/results_test0.csv\"\n",
    "#y_test_results_path = \"/home/ubuntu/anaconda3/results_test_small.csv\"\n",
    "results_test2 = pd.read_csv(y_test_results_path)\n",
    "del results_test2['Unnamed: 0']\n",
    "results_test2['revenue_string'] = results_test2['revenue'].astype(str)\n",
    "\n",
    "y_test_path = \"/home/ubuntu/anaconda3/Y_final.csv\"\n",
    "y_test = pd.read_csv(y_test_path)\n",
    "y_test['Revenue_string'] = y_test['Revenue'].astype(str)\n",
    "y_test['True_label'] = 2\n",
    "del y_test['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_test = results_test2.merge(y_test, left_on=['CIK','revenue_string'], right_on=['CIK','Revenue_string'], how='left')\n",
    "Results_test = Results_test[['period', 'revenue', 'doc', 'table', 'probability', 'True_label']]\n",
    "Results_test = Results_test.fillna(1)\n",
    "\n",
    "Results_test = Results_test.sort_values(['doc', 'probability'], ascending=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aux = []\n",
    "Aux.append(1)\n",
    "test_score = []\n",
    "\n",
    "for i in range(1, len(Results_test)):\n",
    "    if Results_test['doc'][i] == Results_test['doc'][i-1]:\n",
    "        Aux.append(Aux[i-1]+1)\n",
    "    else:\n",
    "        Aux.append(1)\n",
    "\n",
    "for i in range(1, len(Aux)):\n",
    "    if Aux[i] in range(1, 10):\n",
    "        test_score.append(2)\n",
    "    else:\n",
    "        test_score.append(1)\n",
    "test_score.append(1)\n",
    "\n",
    "test_score = pd.DataFrame(test_score)        \n",
    "Results_test = pd.concat([Results_test, test_score], axis=1)\n",
    "Results_test.rename(columns={0:'test_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = []\n",
    "TN = []\n",
    "FP = []\n",
    "FN = []\n",
    "\n",
    "for i in range(len(Results_test)):\n",
    "    if Results_test['True_label'][i] == 2:\n",
    "        if Results_test['test_score'][i] == 2:\n",
    "            TP.append(1)\n",
    "            TN.append(0)\n",
    "            FP.append(0)\n",
    "            FN.append(0)\n",
    "        else:\n",
    "            TP.append(0)\n",
    "            TN.append(0)\n",
    "            FP.append(0)\n",
    "            FN.append(1)\n",
    "    else:\n",
    "        if Results_test['test_score'][i] == 2:\n",
    "            TP.append(0)\n",
    "            TN.append(0)\n",
    "            FP.append(1)\n",
    "            FN.append(0)\n",
    "        else:\n",
    "            TP.append(0)\n",
    "            TN.append(1)\n",
    "            FP.append(0)\n",
    "            FN.append(0)\n",
    "\n",
    "TP = pd.DataFrame(TP)\n",
    "TN = pd.DataFrame(TN)\n",
    "FP = pd.DataFrame(FP)\n",
    "FN = pd.DataFrame(FN)\n",
    "Results_test = pd.concat([Results_test, TP, TN, FP, FN], axis=1)\n",
    "Results_test.columns = ['period','revenue','doc','table','probability','True_label','test_score','TP','TN','FP','FN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in the test set is 30.1%\n",
      "The precision in the test set is 26.1%\n",
      "The recall in the test set is 87.0%\n",
      "The F1 score in the test set is 40.1%\n"
     ]
    }
   ],
   "source": [
    "TP = Results_test['TP'].sum()\n",
    "TN = Results_test['TN'].sum()\n",
    "FP = Results_test['FP'].sum()\n",
    "FN = Results_test['FN'].sum()\n",
    "\n",
    "Accuracy = (TP+TN)/len(Results_test)\n",
    "Precision = TP / (TP+FP)\n",
    "Recall = TP / (TP+FN)\n",
    "F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "print(\"The accuracy in the test set is %.1f%%\" % (100*Accuracy))\n",
    "print(\"The precision in the test set is %.1f%%\" % (100*Precision))\n",
    "print(\"The recall in the test set is %.1f%%\" % (100*Recall))\n",
    "print(\"The F1 score in the test set is %.1f%%\" % (100*F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_results_path = \"/home/ubuntu/anaconda3/results_train0.csv\"\n",
    "results_train2 = pd.read_csv(y_train_results_path)\n",
    "#results_dev2 = results_dev\n",
    "del results_train2['Unnamed: 0']\n",
    "results_train2['revenue_string'] = results_train2['revenue'].astype(str)\n",
    "\n",
    "y_train_path = \"/home/ubuntu/anaconda3/Y_final.csv\"\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "y_train['Revenue_string'] = y_train['Revenue'].astype(str)\n",
    "y_train['True_label'] = 2\n",
    "del y_train['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_train = results_train2.merge(y_train, left_on=['CIK','revenue_string'], right_on=['CIK','Revenue_string'], how='left')\n",
    "Results_train = Results_train[['period', 'revenue', 'doc', 'table', 'probability', 'True_label']]\n",
    "Results_train = Results_train.fillna(1)\n",
    "\n",
    "Results_train = Results_train.sort_values(['doc', 'probability'], ascending=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aux = []\n",
    "Aux.append(1)\n",
    "train_score = []\n",
    "\n",
    "for i in range(1, len(Results_train)):\n",
    "    if Results_train['doc'][i] == Results_train['doc'][i-1]:\n",
    "        Aux.append(Aux[i-1]+1)\n",
    "    else:\n",
    "        Aux.append(1)\n",
    "\n",
    "for i in range(1, len(Aux)):\n",
    "    if Aux[i] in range(1, 10):\n",
    "        train_score.append(2)\n",
    "    else:\n",
    "        train_score.append(1)\n",
    "train_score.append(1)\n",
    "\n",
    "train_score = pd.DataFrame(train_score)        \n",
    "Results_train = pd.concat([Results_train, train_score], axis=1)\n",
    "Results_train.rename(columns={0:'train_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = []\n",
    "TN = []\n",
    "FP = []\n",
    "FN = []\n",
    "\n",
    "for i in range(len(Results_train)):\n",
    "    if Results_train['True_label'][i] == 2:\n",
    "        if Results_train['train_score'][i] == 2:\n",
    "            TP.append(1)\n",
    "            TN.append(0)\n",
    "            FP.append(0)\n",
    "            FN.append(0)\n",
    "        else:\n",
    "            TP.append(0)\n",
    "            TN.append(0)\n",
    "            FP.append(0)\n",
    "            FN.append(1)\n",
    "    else:\n",
    "        if Results_train['train_score'][i] == 2:\n",
    "            TP.append(0)\n",
    "            TN.append(0)\n",
    "            FP.append(1)\n",
    "            FN.append(0)\n",
    "        else:\n",
    "            TP.append(0)\n",
    "            TN.append(1)\n",
    "            FP.append(0)\n",
    "            FN.append(0)\n",
    "\n",
    "TP = pd.DataFrame(TP)\n",
    "TN = pd.DataFrame(TN)\n",
    "FP = pd.DataFrame(FP)\n",
    "FN = pd.DataFrame(FN)\n",
    "Results_train = pd.concat([Results_train, TP, TN, FP, FN], axis=1)\n",
    "Results_train.columns = ['period','revenue','doc','table','probability','True_label','train_score','TP','TN','FP','FN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in the train set is 26.4%\n",
      "The precision in the train set is 24.5%\n",
      "The recall in the train set is 98.3%\n",
      "The F1 score in the train set is 39.2%\n"
     ]
    }
   ],
   "source": [
    "TP = Results_train['TP'].sum()\n",
    "TN = Results_train['TN'].sum()\n",
    "FP = Results_train['FP'].sum()\n",
    "FN = Results_train['FN'].sum()\n",
    "\n",
    "Accuracy = (TP+TN)/len(Results_train)\n",
    "Precision = TP / (TP+FP)\n",
    "Recall = TP / (TP+FN)\n",
    "F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "print(\"The accuracy in the train set is %.1f%%\" % (100*Accuracy))\n",
    "print(\"The precision in the train set is %.1f%%\" % (100*Precision))\n",
    "print(\"The recall in the train set is %.1f%%\" % (100*Recall))\n",
    "print(\"The F1 score in the train set is %.1f%%\" % (100*F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train_dense = L_train[0].todense()\n",
    "\n",
    "pickle_out = open(\"pickle.L_Train_full\",\"wb\")\n",
    "pickle.dump(L_train_dense, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev_dense = L_dev[0].todense()\n",
    "\n",
    "pickle_out = open(\"pickle.L_Dev_full\",\"wb\")\n",
    "pickle.dump(L_dev_dense, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_test_dense = L_test[0].todense()\n",
    "\n",
    "pickle_out = open(\"pickle.L_Test_full\",\"wb\")\n",
    "pickle.dump(L_test_dense, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_train_dense = F_train[0].todense()\n",
    "\n",
    "pickle_out = open(\"pickle.F_Train_full\",\"wb\")\n",
    "pickle.dump(F_train_dense, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_dev_dense = F_dev[0].todense()\n",
    "\n",
    "pickle_out = open(\"pickle.F_dev_full\",\"wb\")\n",
    "pickle.dump(F_dev_dense, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_test_dense = F_test[0].todense()\n",
    "\n",
    "pickle_out = open(\"pickle.F_test_full\",\"wb\")\n",
    "pickle.dump(F_test_dense, pickle_out, protocol=4)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_custom_marginals(results_pandas):\n",
    "\n",
    "    cols = list(results_pandas)\n",
    "    results_pandas['score'] = 0\n",
    "    results_pandas['prob'] = 0\n",
    "    LF_names = cols[5:(len(cols)-1)]\n",
    "    doc_names = results_pandas.doc.unique()\n",
    "\n",
    "    for col in LF_names:\n",
    "        for row in range(len(results_pandas)):\n",
    "            if results_pandas[col][row] == 2: \n",
    "                results_pandas.loc[row, 'score'] += 1\n",
    "            elif results_pandas[col][row] == 1:\n",
    "                results_pandas.loc[row, 'score'] -= 1\n",
    "\n",
    "\n",
    "    #calculating means\n",
    "    totals = {}\n",
    "    count = {}\n",
    "    means = {}\n",
    "\n",
    "    for row in range(len(results_pandas)):\n",
    "        doc = results_pandas['doc'][row]\n",
    "        if doc in totals:\n",
    "            totals[doc] += results_pandas['score'][row]\n",
    "            count[doc] += 1\n",
    "        else:\n",
    "            totals[doc] = results_pandas['score'][row]\n",
    "            count[doc] = 1\n",
    "\n",
    "    for doc in doc_names:\n",
    "        means[doc] = totals[doc] / count[doc]\n",
    "\n",
    "\n",
    "    #calculating stdevs\n",
    "    stdev = {}\n",
    "\n",
    "    for row in range(len(results_pandas)):\n",
    "        doc = results_pandas['doc'][row]\n",
    "        if doc in stdev:\n",
    "            stdev[doc] += (results_pandas['score'][row] - means[doc])**2\n",
    "        else:\n",
    "            stdev[doc] = (results_pandas['score'][row] - means[doc])**2\n",
    "\n",
    "    for doc in doc_names:\n",
    "        stdev[doc] = math.sqrt(stdev[doc] / count[doc])\n",
    "\n",
    "    #calculating probabilities\n",
    "    for row in range(len(results_pandas)):\n",
    "        doc = results_pandas['doc'][row]\n",
    "        if stdev[doc] == 0:\n",
    "            results_pandas.loc[row, 'prob'] = 0.5\n",
    "        else:\n",
    "            intermediate = (results_pandas['score'][row] - means[doc]) / stdev[doc]\n",
    "            results_pandas.loc[row, 'prob'] = 1 / (1 + np.exp(-intermediate))\n",
    "    \n",
    "    results_pandas.fillna(0.5)\n",
    "    probs = results_pandas['prob'].values\n",
    "    probs = np.reshape(probs, (probs.shape[0],1))\n",
    "    probs_1 = 1 - results_pandas['prob'].values\n",
    "    probs_1 = np.reshape(probs_1, (probs_1.shape[0],1))\n",
    "    new_marginals = np.concatenate((probs_1, probs), axis = 1)\n",
    "    \n",
    "    return new_marginals, results_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cik(pandas_df):\n",
    "    \n",
    "    documents = pandas_df['doc']\n",
    "    cik_list = [0] * len(documents)\n",
    "    i = 0 \n",
    "    Aux = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        document_name = doc.name\n",
    "        count_lowdash = 0\n",
    "        count_word = 0\n",
    "        cik =''\n",
    "        for  letter in document_name:\n",
    "            count_word += 1\n",
    "            if letter == \"_\": count_lowdash += 1\n",
    "            if count_word >= 10 & count_lowdash == 0:\n",
    "                cik = cik + letter\n",
    "        cik = cik[:-5]\n",
    "        Aux.append(cik)\n",
    "        i += 1\n",
    "        \n",
    "    Aux = pd.DataFrame(Aux)        \n",
    "    pandas_df = pd.concat([pandas_df, Aux], axis=1)\n",
    "    pandas_df.rename(columns={0:'CIK'}, inplace=True)\n",
    "        \n",
    "    return pandas_df\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl2] *",
   "language": "python",
   "name": "conda-env-dl2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
